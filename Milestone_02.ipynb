{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone Two: Modeling and Feature Engineering\n",
    "\n",
    "### Due: Midnight on April 13 (with 2-hour grace period) and worth 25 points\n",
    "\n",
    "### Overview\n",
    "\n",
    "This milestone builds on your work from Milestone 1. You will:\n",
    "\n",
    "1. Evaluate baseline models using default settings.\n",
    "2. Engineer new features and re-evaluate models.\n",
    "3. Use feature selection techniques to find promising subsets.\n",
    "4. Select the top 3 models and fine-tune them for optimal performance.\n",
    "\n",
    "You must do all work in this notebook and upload to your team leader's account in Gradescope. There is no\n",
    "Individual Assessment for this Milestone. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================\n",
    "# Useful Imports: Add more as needed\n",
    "# ===================================\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from itertools import chain, combinations\n",
    "from math import sqrt\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.ticker as mticker  # Optional: Format y-axis labels as dollars\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-learn (Machine Learning)\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, \n",
    "    cross_val_score, \n",
    "    GridSearchCV, \n",
    "    RandomizedSearchCV, \n",
    "    RepeatedKFold\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, f_regression, SelectKBest\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Progress Tracking\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =============================\n",
    "# Global Variables\n",
    "# =============================\n",
    "random_state = 42\n",
    "\n",
    "# =============================\n",
    "# Utility Functions\n",
    "# =============================\n",
    "\n",
    "# Format y-axis labels as dollars with commas (optional)\n",
    "def dollar_format(x, pos):\n",
    "    return f'${x:,.0f}'\n",
    "\n",
    "# Convert seconds to HH:MM:SS format\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prelude: Load your Preprocessed Dataset from Milestone 1\n",
    "\n",
    "In Milestone 1, you handled missing values, encoded categorical features, and explored your data. Before you begin this milestone, you’ll need to load that cleaned dataset and prepare it for modeling.\n",
    "\n",
    "Here’s what to do:\n",
    "\n",
    "1. Return to your Milestone 1 notebook and rerun your code through Part 3, where your dataset was fully cleaned (assume it’s called `df_cleaned`).\n",
    "\n",
    "2. **Save** the cleaned dataset to a file by running:\n",
    "\n",
    ">   df_cleaned.to_csv(\"zillow_cleaned.csv\", index=False)\n",
    "\n",
    "3. Switch to this notebook and **load** the saved data:\n",
    "\n",
    ">   df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "\n",
    "4. Create a **train/test split** using `train_test_split`.  \n",
    "   \n",
    "6. **Standardize** the features (but not the target!) using **only the training data.** This ensures consistency across models without introducing data leakage from the test set:\n",
    "\n",
    ">   scaler = StandardScaler()   \n",
    ">   X_train_scaled = scaler.fit_transform(X_train)    \n",
    "  \n",
    "**Notes:** \n",
    "\n",
    "- You will not use the testing set during this milestone — it’s reserved for final evaluation later.\n",
    "- You will have to redo the scaling step when you introduce new features (which have to be scaled as well).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parcelid</th>\n",
       "      <th>airconditioningtypeid</th>\n",
       "      <th>bathroomcnt</th>\n",
       "      <th>bedroomcnt</th>\n",
       "      <th>buildingqualitytypeid</th>\n",
       "      <th>calculatedbathnbr</th>\n",
       "      <th>calculatedfinishedsquarefeet</th>\n",
       "      <th>finishedsquarefeet12</th>\n",
       "      <th>fireplacecnt</th>\n",
       "      <th>fullbathcnt</th>\n",
       "      <th>...</th>\n",
       "      <th>roomcnt</th>\n",
       "      <th>threequarterbathnbr</th>\n",
       "      <th>unitcnt</th>\n",
       "      <th>yearbuilt</th>\n",
       "      <th>numberofstories</th>\n",
       "      <th>assessmentyear</th>\n",
       "      <th>taxdelinquencyflag</th>\n",
       "      <th>taxdelinquencyyear</th>\n",
       "      <th>censustractandblock</th>\n",
       "      <th>taxvaluedollarcnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14297519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>3100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.059063e+13</td>\n",
       "      <td>1023282.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17052889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.111001e+13</td>\n",
       "      <td>464000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14186244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1962.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.059022e+13</td>\n",
       "      <td>564778.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12177905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>2376.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1970.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.037300e+13</td>\n",
       "      <td>145143.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10887214</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>1312.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.037124e+13</td>\n",
       "      <td>119407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   parcelid  airconditioningtypeid  bathroomcnt  bedroomcnt  \\\n",
       "0  14297519                    0.0          3.5         4.0   \n",
       "1  17052889                    0.0          1.0         2.0   \n",
       "2  14186244                    0.0          2.0         3.0   \n",
       "3  12177905                    0.0          3.0         4.0   \n",
       "4  10887214                    1.0          3.0         3.0   \n",
       "\n",
       "   buildingqualitytypeid  calculatedbathnbr  calculatedfinishedsquarefeet  \\\n",
       "0                    0.0                3.5                        3100.0   \n",
       "1                    0.0                1.0                        1465.0   \n",
       "2                    0.0                2.0                        1243.0   \n",
       "3                    8.0                3.0                        2376.0   \n",
       "4                    8.0                3.0                        1312.0   \n",
       "\n",
       "   finishedsquarefeet12  fireplacecnt  fullbathcnt  ...  roomcnt  \\\n",
       "0                3100.0           0.0          3.0  ...      0.0   \n",
       "1                1465.0           1.0          1.0  ...      5.0   \n",
       "2                1243.0           0.0          2.0  ...      6.0   \n",
       "3                2376.0           0.0          3.0  ...      0.0   \n",
       "4                1312.0           0.0          3.0  ...      0.0   \n",
       "\n",
       "   threequarterbathnbr unitcnt  yearbuilt  numberofstories  assessmentyear  \\\n",
       "0                  1.0     1.0     1998.0              1.0          2016.0   \n",
       "1                  0.0     1.0     1967.0              1.0          2016.0   \n",
       "2                  0.0     1.0     1962.0              1.0          2016.0   \n",
       "3                  0.0     1.0     1970.0              1.0          2016.0   \n",
       "4                  0.0     1.0     1964.0              1.0          2016.0   \n",
       "\n",
       "  taxdelinquencyflag  taxdelinquencyyear  censustractandblock  \\\n",
       "0                  0                 0.0         6.059063e+13   \n",
       "1                  0                 0.0         6.111001e+13   \n",
       "2                  0                 0.0         6.059022e+13   \n",
       "3                  0                 0.0         6.037300e+13   \n",
       "4                  0                 0.0         6.037124e+13   \n",
       "\n",
       "   taxvaluedollarcnt  \n",
       "0          1023282.0  \n",
       "1           464000.0  \n",
       "2           564778.0  \n",
       "3           145143.0  \n",
       "4           119407.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"zillow_cleaned.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target and features\n",
    "target = \"taxvaluedollarcnt\"\n",
    "X = df.drop(columns=[\"taxvaluedollarcnt\", \"parcelid\"])  # drop ID and target\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    25\n",
       "object      3\n",
       "int64       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric columns: ['hashottuborspa', 'propertycountylandusecode', 'taxdelinquencyflag']\n"
     ]
    }
   ],
   "source": [
    "non_numeric_cols = X.select_dtypes(include=['object']).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_cols.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only numeric features\n",
    "X_numeric = X.select_dtypes(include=[\"int64\", \"float64\"])\n",
    "\n",
    "# Drop columns that have any missing values\n",
    "X_numeric_clean = X_numeric.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_numeric_clean, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and apply scaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Baseline Modeling [3 pts]\n",
    "\n",
    "Apply the following regression models to the scaled training dataset using **default parameters**:\n",
    "\n",
    "- Linear Regression\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Decision Tree Regression\n",
    "- Bagging\n",
    "- Random Forest\n",
    "- Gradient Boosting Trees\n",
    "\n",
    "For each model:\n",
    "- Use **repeated cross-validation** (e.g., 5 folds, 5 repeats).\n",
    "- Report the **mean and standard deviation of CV RMSE Score** across all folds in a table. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.511e+15, tolerance: 1.736e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.468e+15, tolerance: 1.629e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.590e+15, tolerance: 1.774e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.495e+15, tolerance: 1.689e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.549e+15, tolerance: 1.775e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.605e+15, tolerance: 1.781e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e+15, tolerance: 1.733e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.473e+15, tolerance: 1.664e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.547e+15, tolerance: 1.743e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+15, tolerance: 1.682e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.574e+15, tolerance: 1.754e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.497e+15, tolerance: 1.691e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.170e+15, tolerance: 1.635e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.571e+15, tolerance: 1.774e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.585e+15, tolerance: 1.749e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e+15, tolerance: 1.692e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.557e+15, tolerance: 1.717e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.977e+15, tolerance: 1.676e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.517e+15, tolerance: 1.745e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.597e+15, tolerance: 1.773e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.342e+15, tolerance: 1.709e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.546e+15, tolerance: 1.726e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.577e+15, tolerance: 1.759e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.509e+15, tolerance: 1.686e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.528e+15, tolerance: 1.722e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# RMSE scorer\n",
    "rmse_scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# Repeated K-Fold Cross-Validation\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# Models with default parameters\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Bagging\": BaggingRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Collect results\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train_scaled,\n",
    "        y_train,\n",
    "        scoring=rmse_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean RMSE\": sqrt(np.mean(scores)),\n",
    "        \"Std RMSE\": sqrt(np.std(scores))\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Comparison (RMSE):\n",
      "            Model  Mean RMSE  Std RMSE\n",
      "    Random Forest     384189    130253\n",
      "Gradient Boosting     393832    135515\n",
      "          Bagging     400214    131294\n",
      " Ridge Regression     456276    158926\n",
      " Lasso Regression     517518    378927\n",
      "    Decision Tree     538414    159793\n",
      "Linear Regression     714829    846157\n"
     ]
    }
   ],
   "source": [
    "# Display Results\n",
    "results_df = pd.DataFrame(results).sort_values(by=\"Mean RMSE\")\n",
    "results_df_clean = results_df.copy()\n",
    "results_df_clean[\"Mean RMSE\"] = results_df_clean[\"Mean RMSE\"].round(0).astype(int)\n",
    "results_df_clean[\"Std RMSE\"] = results_df_clean[\"Std RMSE\"].round(0).astype(int)\n",
    "\n",
    "print(\"Baseline Model Comparison (RMSE):\")\n",
    "print(results_df_clean.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Discussion [2 pts]\n",
    "\n",
    "In a paragraph or well-organized set of bullet points, briefly compare and discuss:\n",
    "\n",
    "  - Which models perform best overall?\n",
    "  - Which are most stable (lowest std)?\n",
    "  - Any signs of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To establish a benchmark for future improvements, we began our modeling process by evaluating a set of baseline regression models using default hyperparameters. These models were trained on the preprocessed and standardized training dataset and assessed using repeated cross-validation (5 folds, 5 repeats) to ensure stability and robustness. The primary evaluation metric was Root Mean Squared Error (RMSE), which reflects the average magnitude of prediction errors in the same units as the target variable (property tax value in dollars).  \n",
    ">  \n",
    "> - The Random Forest model had **the best overall** performance with the lowest mean RMSE (384,255), followed closely by Gradient Boosting and Bagging, making ensemble methods the top performers on this dataset.\n",
    "> - In terms of stability, all three ensemble models (Random Forest, Gradient Boosting, Bagging) also showed **low standard deviation**, indicating that their performance was consistent across different folds.\n",
    "> - Linear Regression performed **the worst overall** with the highest RMSE (611,779) and a very high standard deviation (371,134), suggesting high variability and potential underfitting. This likely reflects its inability to capture non-linear relationships in the data.\n",
    "> - Lasso Regression also showed **poor performance**, especially in stability, with an extremely high std (116,851), indicating it might not be selecting useful features effectively with the default alpha.\n",
    "> - Decision Tree had high RMSE and moderate std, suggesting possible **overfitting** as it tends to model noise without ensemble regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Feature Engineering [3 pts]\n",
    "\n",
    "Consider **at least three new features** based on your Milestone 1, Part 5. Examples include:\n",
    "- Polynomial terms\n",
    "- Log or interaction terms\n",
    "- Groupings or transformations of categorical features\n",
    "\n",
    "Add these features to `X_train` and then:\n",
    "- Scale using `StandardScaler` \n",
    "- Re-run all models listed above (using default settings again).\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table. \n",
    "\n",
    "**Note:**  Recall that this will require creating a new version of the dataset, so effectively you may be running \"polynomial regression\" using `LinearRegression`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.090e+15, tolerance: 1.736e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.923e+15, tolerance: 1.629e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.183e+15, tolerance: 1.774e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.904e+15, tolerance: 1.689e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.071e+15, tolerance: 1.775e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.176e+15, tolerance: 1.781e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.029e+15, tolerance: 1.733e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.868e+15, tolerance: 1.664e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.008e+15, tolerance: 1.682e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.082e+15, tolerance: 1.743e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e+15, tolerance: 1.691e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+15, tolerance: 1.754e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.829e+15, tolerance: 1.635e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+15, tolerance: 1.774e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.165e+15, tolerance: 1.749e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.995e+15, tolerance: 1.692e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.118e+15, tolerance: 1.717e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.859e+15, tolerance: 1.676e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.008e+15, tolerance: 1.745e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.198e+15, tolerance: 1.773e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.043e+15, tolerance: 1.726e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.948e+15, tolerance: 1.709e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.158e+15, tolerance: 1.759e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.035e+15, tolerance: 1.686e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.991e+15, tolerance: 1.722e+12\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Add as many code cells as you need\n",
    "from sklearn.preprocessing import PolynomialFeatures, FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "log_transformer = FunctionTransformer(func=np.log1p, validate=True)\n",
    "\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_train_log = log_transformer.fit_transform(X_train)\n",
    "#interaction_terms = X_train.iloc[:, 0] * X_train.iloc[:, 1]\n",
    "#X_train_new = np.hstack((X_train, X_train_poly, X_train_log, interaction_terms.values.reshape(-1, 1)))\n",
    "\n",
    "# Combine original features with new ones\n",
    "X_train_new = np.hstack((X_train, X_train_poly, X_train_log))\n",
    "\n",
    "# Scale features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_new = scaler.fit_transform(X_train_new)\n",
    "\n",
    "results_new = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        X_train_scaled_new,\n",
    "        y_train,\n",
    "        scoring=rmse_scorer,\n",
    "        cv=cv,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    results_new.append({\n",
    "        \"Model\": name,\n",
    "        \"Mean RMSE\": np.sqrt(np.mean(scores)),\n",
    "        \"Std RMSE\": np.sqrt(np.std(scores))\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_new)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Discussion [2 pts]\n",
    "\n",
    "Reflect on the impact of your new features:\n",
    "\n",
    "- Did any models show notable improvement in performance?\n",
    "\n",
    "- Which new features seemed to help — and in which models?\n",
    "\n",
    "- Do you have any hypotheses about why a particular feature helped (or didn’t)?\n",
    "\n",
    "- Were there any unexpected results?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Feature Selection [3 pts]\n",
    "\n",
    "Using the full set of features (original + engineered):\n",
    "- Apply **feature selection** methods to investigate whether you can improve performance.\n",
    "  - You may use forward selection, backward selection, or feature importance from tree-based models.\n",
    "- For each model, identify the **best-performing subset of features**.\n",
    "- Re-run each model using only those features.\n",
    "- Report updated RMSE scores (mean and std) across repeated CV in a table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Feature Helper\n",
    "\n",
    "def forward_feature_selection(X, y, model, \n",
    "                              scoring='neg_mean_squared_error', \n",
    "                              cv=5, \n",
    "                              tol=None,               # None = no delta cutoff\n",
    "                                                      # use 0.0 for \"no further improvements\"\n",
    "                                                      # and 1e-4 for \"point of diminishing returns\"                                      \n",
    "                              max_features=None,      # None = use all features\n",
    "                              n_jobs=-1,\n",
    "                              verbose=False\n",
    "                             ):\n",
    "    selected_features = []                            # List to store the order of features selected\n",
    "    remaining_features = list(X.columns)              # Features not yet selected\n",
    "    best_scores = []                                  # List to store the CV score after each feature addition\n",
    "    previous_score = float('inf')                     # Initialize previous score for improvement comparison\n",
    "\n",
    "    # Track the best subset of features and its corresponding score\n",
    "    \n",
    "    best_feature_set = None                           # Best combination of features found so far\n",
    "    best_score = float('inf')                         # Best CV score observed so far\n",
    "    \n",
    "    while remaining_features:\n",
    "        scores = {}                                   # Dictionary to hold CV scores for each candidate feature\n",
    "        for feature in remaining_features:\n",
    "            current_features = selected_features + [feature]\n",
    "            \n",
    "            # Compute the CV score for the current set of features (negated MSE, so lower is better)\n",
    "            # EDIT: Compute the RMSE here scoring alread uses \"neg mean squared\"\n",
    "            cv_score = np.sqrt(-cross_val_score(model, X[current_features], y, \n",
    "                                        scoring=scoring, cv=cv, n_jobs=n_jobs\n",
    "                                       ).mean())\n",
    "            scores[feature] = cv_score\n",
    "\n",
    "        # Select the feature that minimizes the CV score\n",
    "        best_feature = min(scores, key=scores.get)\n",
    "        current_score = scores[best_feature]\n",
    "            \n",
    "        # Check if the improvement is significant based on the tolerance (tol)\n",
    "        if tol is not None and previous_score - current_score < tol:\n",
    "            if verbose:\n",
    "                print(\"Stopping early due to minimal improvement.\")\n",
    "            break\n",
    "\n",
    "        # Add the best feature to the selected list and update score trackers\n",
    "        selected_features.append(best_feature)\n",
    "        best_scores.append(current_score)\n",
    "        remaining_features.remove(best_feature)\n",
    "        previous_score = current_score\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nFeatures: {selected_features[-3:]}, CV Score (RMSE): {current_score:.4f}\")\n",
    "        \n",
    "        # Update the best subset if the current score is better than the best so far\n",
    "        if current_score < best_score:\n",
    "            best_score = current_score\n",
    "            best_feature_set = selected_features.copy()\n",
    "        \n",
    "        # Check if the maximum number of features has been reached\n",
    "        if max_features is not None and len(selected_features) >= max_features:\n",
    "            break\n",
    "\n",
    "    return (\n",
    "        selected_features,      # List of features in the order they were selected (this will be ALL features if max_features == None\n",
    "        best_scores,            # List of cross-validation scores corresponding to each addition in the previous list\n",
    "        best_feature_set,       # The subset of features that achieved the best CV score.\n",
    "        best_score              # The best CV score\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Discussion [2 pts]\n",
    "\n",
    "Analyze the effect of feature selection on your models:\n",
    "\n",
    "- Did performance improve for any models after reducing the number of features?\n",
    "\n",
    "- Which features were consistently retained across models?\n",
    "\n",
    "- Were any of your newly engineered features selected as important?\n",
    "\n",
    "- How did feature selection differ between linear and tree-based models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Fine-Tuning Your Top 3 Models [6 pts]\n",
    "\n",
    "In this final phase of Milestone 2, you’ll select and refine your **three most promising models and their corresponding data pipelines** based on everything you've done so far.\n",
    "\n",
    "1. Choose the top 3 models based on performance and interpretability from earlier parts.\n",
    "2. For each model:\n",
    "   - Perform hyperparameter tuning using `sweep_parameters`, `GridSearchCV`, `RandomizedSearchCV`, or other techniques from previous homeworks. \n",
    "   - Experiment with different versions of your feature engineering and preprocessing — treat these as additional tunable components.\n",
    "3. Report the mean and standard deviation of CV RMSE score for each model in a summary table.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add as many code cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Discussion [4 pts]\n",
    "\n",
    "Reflect on your tuning process and final results:\n",
    "\n",
    "- What was your tuning strategy for each model? Why did you choose those hyperparameters?\n",
    "- Did you find that certain types of preprocessing or feature engineering worked better with specific models?\n",
    "- Provide a ranking of your three models and explain your reasoning — not just based on RMSE, but also interpretability, training time, or generalizability.\n",
    "- Conclude by considering whether this workflow has produced the results you expected. Typically, you would repeat steps 2 - 4 and also reconsider the choices you made in Milestone 1 when cleaning the dataset, until reaching the point of diminishing returns; do you think that would that have helped here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Your text here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
